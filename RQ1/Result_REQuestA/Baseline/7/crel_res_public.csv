metric,score,evaluation_model,reason,error
Contextual Relevancy,1.0,Llama-3 70B,"The score is 1.00 because the input is perfectly matched with the context, and there are no reasons for irrelevancy provided, so it",
Contextual Relevancy,1.0,Llama-3 70B,"The score is 1.00 because there is no irrelevant information provided, and the context perfectly aligns with the input, making it a perfect match! Well done!",
Contextual Relevancy,1.0,Llama-3 70B,"The score is 1.00 because the context is perfectly aligned with the input, providing a precise answer to the question asked.",
Contextual Relevancy,0.6666666666666666,Llama-3 70B,"The score is 0.67 because the context is unrelated to the input question, specifically discussing the OPENCOSS platform, evidence management, and safety standards, which are not relevant to authorities requesting new evidence for safety criteria.",
Contextual Relevancy,0.6666666666666666,Llama-3 70B,"The score is 0.67 because the retrieval context is not provided, making it impossible to determine its relevance to the input, resulting in a mediocre score.",
Contextual Relevancy,1.0,Llama-3 70B,"The score is 1.00 because the input is a general question and the retrieval context is empty, indicating a perfect match.",
Contextual Relevancy,1.0,Llama-3 70B,"The score is 1.00 because the input is a clear and specific question, and the retrieval context is perfectly aligned to provide a relevant response, making it a perfect match!",
Contextual Relevancy,0.5,Llama-3 70B,,
Contextual Relevancy,1.0,Llama-3 70B,"The score is 1.00 because the input is perfectly aligned with the retrieval context, making it a seamless match!",
Contextual Relevancy,1.0,Llama-3 70B,"The score is 1.00 because this context perfectly aligns with the input, providing a precise and accurate response to the question about flaws in function level requirements.",
Contextual Relevancy,1.0,Llama-3 70B,"The score is 1.00 because the actual output perfectly addresses the input question, providing a clear and concise answer without any irrelevant information. Keep up the great work! ðŸ˜Š",
Contextual Relevancy,1.0,Llama-3 70B,"The score is 1.00 because the input is perfectly clear and the retrieval context is perfectly relevant, a match made in heaven!",
Contextual Relevancy,0.5,Llama-3 70B,"The score is 0.50 because the context does not mention anything about business processes and instead talks about unrelated space-related concepts, such as procedures and telecommands, which do not provide any relevant information about the input.",
Contextual Relevancy,1.0,Llama-3 70B,,
Contextual Relevancy,1.0,Llama-3 70B,"The score is 1.00 because this input is perfectly aligned with the context, and there",
Contextual Relevancy,1.0,Llama-3 70B,,
Contextual Relevancy,1.0,Llama-3 70B,,
Contextual Relevancy,1.0,Llama-3 70B,,
Contextual Relevancy,1.0,Llama-3 70B,"The score is 1.00 because the input is directly related to the context, with no reasons provided for irrelevancy, indicating a perfect match.",
Contextual Relevancy,0.6666666666666666,Llama-3 70B,"The score is 0.67 because there are no reasons provided for why the retrieval context is irrelevant to the input, making it difficult to determine the relevancy score accurately.",
Contextual Relevancy,1.0,Llama-3 70B,"The score is 1.00 because there are no reasons for irrelevancy, indicating a perfect match between the input and retrieval context.",
Contextual Relevancy,1.0,Llama-3 70B,,
Contextual Relevancy,1.0,Llama-3 70B,"The score is 1.00 because the input is perfectly matched with the context, providing an exact match for the query, hence the highest score possible.",
Contextual Relevancy,1.0,Llama-3 70B,,
Contextual Relevancy,1.0,Llama-3 70B,"The score is 1.00 because the input is a clear question, and the retrieval context is empty, indicating a perfect match!",
Contextual Relevancy,1.0,Llama-3 70B,"The score is 1.00 because the input is a direct question asking for a specific piece of information, and the retrieval context directly answers it, making it perfectly relevant.",
Contextual Relevancy,1.0,Llama-3 70B,"The score is 1.00 because the retrieval context is perfectly aligned with the input, providing a precise answer to the question.",
Contextual Relevancy,1.0,Llama-3 70B,"The score is 1.00 because there are no reasons for irrelevancy provided, indicating a perfect match between the input and the context.",
Contextual Relevancy,1.0,Llama-3 70B,"The score is 1.00 because the retrieval context is perfectly aligned with the input, making it extremely relevant.",
Contextual Relevancy,1.0,Llama-3 70B,,
Contextual Relevancy,1.0,Llama-3 70B,"The score is 1.00 because the input is perfectly aligned with the retrieval context, and there are no reasons for irrelevancy provided.",
Contextual Relevancy,1.0,Llama-3 70B,"The score is 1.00 because the retrieval context is perfectly aligned with the input, providing a precise answer to the user",
Contextual Relevancy,1.0,Llama-3 70B,"The score is 1.00 because the input is a clear and concise question, and the retrieval context is perfectly relevant, providing a precise definition of the International Rosetta Mission.",
Contextual Relevancy,1.0,Llama-3 70B,"The score is 1.00 because the retrieval context is perfectly aligned with the input, providing a seamless connection.",
Contextual Relevancy,1.0,Llama-3 70B,"The score is 1.00 because the retrieval context is perfectly aligned with the input, providing a direct answer to the question.",
Contextual Relevancy,0.0,Llama-3 70B,"The score is 0.00 because the context does not mention the specific asteroids the spacecraft will pass close to, instead discussing comets and the formation of the Solar System, making it completely irrelevant to the input question.",
Contextual Relevancy,1.0,Llama-3 70B,"The score is 1.00 because the retrieval context is perfectly aligned with the input, providing a direct answer to the question ",
Contextual Relevancy,1.0,Llama-3 70B,,
Contextual Relevancy,0.3333333333333333,Llama-3 70B,The score is 0.33 because the context only partially addresses the input,
Contextual Relevancy,1.0,Llama-3 70B,,
Contextual Relevancy,1.0,Llama-3 70B,"The score is 1.00 because the input is directly related to the retrieval context, and there are no reasons provided for why it",
Contextual Relevancy,1.0,Llama-3 70B,,
Contextual Relevancy,1.0,Llama-3 70B,"The score is 1.00 because the input is directly addressed in the retrieval context, resulting in a perfect match.",
Contextual Relevancy,1.0,Llama-3 70B,,
Contextual Relevancy,1.0,Llama-3 70B,"The score is 1.00 because the retrieval context is perfectly relevant to the input, and there",
Contextual Relevancy,1.0,Llama-3 70B,"The score is 1.00 because the retrieval context is perfectly relevant to the input, providing an exact match!",
Contextual Relevancy,0.6666666666666666,Llama-3 70B,"The score is 0.67 because the context is about OPENCOSS platform, which is unrelated to the input",
Contextual Relevancy,0.0,Llama-3 70B,The score is 0.00 because there is no relevant information provided to support the input.,
Contextual Relevancy,1.0,Llama-3 70B,"The score is 1.00 because the retrieval context is perfectly aligned with the input, providing a direct answer to the question asked.",
Contextual Relevancy,1.0,Llama-3 70B,"The score is 1.00 because the retrieval context perfectly aligns with the input, providing a direct answer to the question asked.",
Contextual Relevancy,0.0,Llama-3 70B,,
Contextual Relevancy,1.0,Llama-3 70B,,
Contextual Relevancy,0.0,Llama-3 70B,The score is 0.00 because the context discusses OPENCOSS platform,
Contextual Relevancy,1.0,Llama-3 70B,"The score is 1.00 because the input is directly answered by the retrieval context, making it perfectly relevant.",
Contextual Relevancy,1.0,Llama-3 70B,"The score is 1.00 because the input and context are perfectly aligned, providing a seamless connection.",
Contextual Relevancy,1.0,Llama-3 70B,"The score is 1.00 because the retrieval context is perfectly aligned with the input, providing a precise match for the question asked.",
Contextual Relevancy,1.0,Llama-3 70B,"The score is 1.00 because the retrieval context perfectly aligns with the input, providing the exact information required.",
Contextual Relevancy,1.0,Llama-3 70B,"The score is 1.00 because the retrieval context is perfectly aligned with the input, providing a direct answer to the question.",
Contextual Relevancy,0.5,Llama-3 70B,"The score is 0.50 because the context is completely unrelated to the input, discussing unrelated topics such as OPENCOSS platform, evidence management, and safety assurance, which do not provide any useful information related to the input question about the Contractor identifying changes to skills and competency required by the RCN.",
Contextual Relevancy,1.0,Llama-3 70B,"The score is 1.00 because the input is directly addressed in the context, making it perfectly relevant.",
Contextual Relevancy,0.0,Llama-3 70B,"The score is 0.00 because there is no relevant information provided in the retrieval context to support the input query, resulting in a complete mismatch.",
Contextual Relevancy,1.0,Llama-3 70B,"The score is 1.00 because the input is directly answered by the retrieval context, making it perfectly relevant.",
Contextual Relevancy,1.0,Llama-3 70B,"The score is 1.00 because the input and retrieval context are perfectly aligned, making the context highly relevant.",
Contextual Relevancy,0.6666666666666666,Llama-3 70B,"The score is 0.67 because the context discusses spacecraft configurations, resources, and systems, which are entirely unrelated to the input query about Formation Commanders and Fleet Commanders, making the context largely irrelevant.",
Contextual Relevancy,1.0,Llama-3 70B,"The score is 1.00 because the retrieval context is perfectly aligned with the input, providing a seamless connection.",
Contextual Relevancy,0.0,Llama-3 70B,"The score is 0.00 because there is no relevant information provided in the retrieval context, making it impossible to establish a connection to the input query.",
Contextual Relevancy,0.5,Llama-3 70B,"The score is 0.50 because the context discusses requirements for the HCCS EG, whereas the input asks about locations for the HCCS EG in Canada, showing a mismatch between the two topics.",
Contextual Relevancy,1.0,Llama-3 70B,"The score is 1.00 because there are no reasons for irrelevancy, indicating a perfect match between the input and the retrieval context.",
Contextual Relevancy,1.0,Llama-3 70B,"The score is 1.00 because the input directly asks about the purpose of Performance Assessment, and the retrieval context is perfectly aligned with the question, providing a precise answer.",
Contextual Relevancy,1.0,Llama-3 70B,"The score is 1.00 because the input and retrieval context are perfectly aligned, with no irrelevant information present, making it a spot-on match.",
Contextual Relevancy,1.0,Llama-3 70B,"The score is 1.00 because the input and retrieval context are perfectly aligned, providing a seamless connection between the two.",
Contextual Relevancy,1.0,Llama-3 70B,"The score is 1.00 because the input is a question that requires a specific answer and the retrieval context is empty, making it perfectly relevant by default.",
Contextual Relevancy,0.6666666666666666,Llama-3 70B,"The score is 0.67 because the context discusses safety and compatibility requirements for the Lunar Exploration Light Rover, but the input asks about the rover",
Contextual Relevancy,0.0,Llama-3 70B,,
Contextual Relevancy,1.0,Llama-3 70B,"The score is 1.00 because the input is directly answered by the context, providing a precise and accurate response.",
Contextual Relevancy,1.0,Llama-3 70B,"The score is 1.00 because there is no evidence of irrelevancy between the input and the context, making them perfectly aligned.",
Contextual Relevancy,1.0,Llama-3 70B,,
Contextual Relevancy,1.0,Llama-3 70B,"The score is 1.00 because the input and context perfectly align, with no irrelevant information provided.",
Contextual Relevancy,1.0,Llama-3 70B,"The score is 1.00 because the input and context are perfectly aligned, with no irrelevant information present, making the context highly relevant to the input.",
Contextual Relevancy,1.0,Llama-3 70B,"The score is 1.00 because the input perfectly matches the context, providing an exact answer to the question asked, which is the ideal scenario for retrieval context relevancy.",
Contextual Relevancy,1.0,Llama-3 70B,"The score is 1.00 because there are no reasons for irrelevancy provided, indicating a perfect match between the input and the retrieval context.",
Contextual Relevancy,0.5,Llama-3 70B,"The score is 0.50 because the context discusses structural Finite Element Models, stability, and verification requirements, but lacks information about the calculations of the Ariane 5 launch vehicle performances, making it unrelated to the input.",
Contextual Relevancy,1.0,Llama-3 70B,"The score is 1.00 because the input and retrieval context are perfectly matched, with no reasons found for irrelevancy.",
Contextual Relevancy,1.0,Llama-3 70B,"The score is 1.00 because the input and context are perfectly aligned, with no signs of irrelevancy whatsoever! Well done!",
Contextual Relevancy,1.0,Llama-3 70B,,
Contextual Relevancy,1.0,Llama-3 70B,,
Contextual Relevancy,1.0,Llama-3 70B,"The score is 1.00 because the retrieval context is perfectly aligned with the input, providing a precise answer to the question asked.",
Contextual Relevancy,1.0,Llama-3 70B,,
Contextual Relevancy,1.0,Llama-3 70B,"The score is 1.00 because the input and retrieval context are perfectly aligned, providing a seamless connection between the question and the answer.",
Contextual Relevancy,1.0,Llama-3 70B,,
Contextual Relevancy,1.0,Llama-3 70B,"The score is 1.00 because the input is perfectly matched with the retrieval context, with no irrelevant information present, resulting in a perfect score.",
Contextual Relevancy,1.0,Llama-3 70B,The score is 1.00 because the input is a specific technical question and there is no irrelevant context to detract from its relevancy.,
Contextual Relevancy,0.3333333333333333,Llama-3 70B,"The score is 0.33 because the context does not mention figures for the entire 10 year mission and instead talks about unrelated topics such as the Rosetta orbiter and its instruments, making it largely irrelevant to the input question.",
Contextual Relevancy,1.0,Llama-3 70B,"The score is 1.00 because the input and retrieval context are perfectly aligned, with no reasons provided for irrelevancy, indicating a seamless match.",
Contextual Relevancy,1.0,Llama-3 70B,"The score is 1.00 because the retrieval context is perfectly aligned with the input, providing a direct answer to the question.",
Contextual Relevancy,1.0,Llama-3 70B,"The score is 1.00 because the input and retrieval context are perfectly aligned, and the context provides a precise answer to the input question, making it highly relevant.",
Contextual Relevancy,1.0,Llama-3 70B,,
Contextual Relevancy,1.0,Llama-3 70B,,
Contextual Relevancy,1.0,Llama-3 70B,"The score is 1.00 because the input is a clear question, and the context is not provided, so it",
Contextual Relevancy,0.6666666666666666,Llama-3 70B,"The score is 0.67 because the context only provides a list of references and citations related to the OPENCOSS platform and evidence management in general, without mentioning the responsible entity or person for managing evidence, making it only partially relevant to the input question.",
Contextual Relevancy,1.0,Llama-3 70B,,
Contextual Relevancy,1.0,Llama-3 70B,"The score is 1.00 because the input perfectly aligns with the retrieval context, and there",
Contextual Relevancy,1.0,Llama-3 70B,,
Contextual Relevancy,1.0,Llama-3 70B,"The score is 1.00 because the retrieval context is perfectly aligned with the input, providing a seamless connection between the two.",
Contextual Relevancy,0.5,Llama-3 70B,"The score is 0.50 because the retrieval context does not provide any information related to the input question, specifically the ",
Contextual Relevancy,0.6666666666666666,Llama-3 70B,"The score is 0.67 because the context appears to be a document title or header with no information about the location of the Lunar Exploration Light Rover, making it only partially relevant to the input.",
Contextual Relevancy,1.0,Llama-3 70B,"The score is 1.00 because this input has no irrelevant context, making it perfectly relevant!",
Contextual Relevancy,1.0,Llama-3 70B,"The score is 1.00 because the input is directly related to the context, and there are no reasons provided for why it",
Contextual Relevancy,1.0,Llama-3 70B,"The score is 1.00 because the input is a specific question requiring a precise answer, and the retrieval context is not provided, making it impossible to determine relevancy.",
Contextual Relevancy,1.0,Llama-3 70B,"The score is 1.00 because the input is a specific question and the retrieval context is not provided, so it",
Contextual Relevancy,1.0,Llama-3 70B,"The score is 1.00 because the retrieval context is perfectly relevant to the input, and we",
Contextual Relevancy,1.0,Llama-3 70B,"The score is 1.00 because the retrieval context is perfectly aligned with the input, providing a precise answer to the question about absolute measurement error.",
Contextual Relevancy,1.0,Llama-3 70B,"The score is 1.00 because the input is a specific, technical question that doesn",
Contextual Relevancy,1.0,Llama-3 70B,"Perfect match! The input directly asks about re-acquiring sun pointing of solar arrays, and there",
Contextual Relevancy,0.5,Llama-3 70B,"The score is 0.50 because the retrieval context is empty, providing no relevant information for the input query.",
Contextual Relevancy,0.5,Llama-3 70B,"The score is 0.50 because the context does not provide the information about what will be used to orbit the comet Wirtanen, which is the specific information requested in the input.",
Contextual Relevancy,0.6666666666666666,Llama-3 70B,"The score is 0.67 because the context talks about comets and asteroids in a scientific context, focusing on their formation and composition, which doesn",
Contextual Relevancy,1.0,Llama-3 70B,"The score is 1.00 because this input is asking for general information and the retrieval context is not needed, making it perfectly relevant.",
Contextual Relevancy,1.0,Llama-3 70B,"The score is 1.00 because the retrieval context is perfectly relevant to the input, providing a precise answer to the question about comet Wirtanen.",
Contextual Relevancy,1.0,Llama-3 70B,"The score is 1.00 because the retrieval context is perfectly relevant to the input, providing the exact answer to the question.",
Contextual Relevancy,0.5,Llama-3 70B,The score is 0.50 because the context discusses comets and asteroids in general terms without mentioning a spacecraft,
Contextual Relevancy,0.5,Llama-3 70B,"The score is 0.50 because there is no retrieval context provided, making it impossible to determine relevancy.",
Contextual Relevancy,0.3333333333333333,Llama-3 70B,The score is 0.33 because the input about ,
Contextual Relevancy,1.0,Llama-3 70B,,
Contextual Relevancy,1.0,Llama-3 70B,,
Contextual Relevancy,0.0,Llama-3 70B,The score is 0.00 because the context does not mention key terms ,
Contextual Relevancy,1.0,Llama-3 70B,"The score is 1.00 because the input is directly answered by the context, making it perfectly relevant.",
Contextual Relevancy,1.0,Llama-3 70B,"The score is 1.00 because the retrieval context perfectly aligns with the input, providing a direct answer to the question asked.",
Contextual Relevancy,1.0,Llama-3 70B,"The score is 1.00 because the input and retrieval context are perfectly aligned, providing a precise match.",
Contextual Relevancy,1.0,Llama-3 70B,,
Contextual Relevancy,0.6666666666666666,Llama-3 70B,,
Contextual Relevancy,0.0,Llama-3 70B,"The score is 0.00 because there is no relevant information in the retrieval context to support the input query, resulting in a complete mismatch.",
Contextual Relevancy,1.0,Llama-3 70B,"The score is 1.00 because the input is a specific question that directly relates to the context, making it perfectly relevant.",
Contextual Relevancy,0.0,Llama-3 70B,"The score is 0.00 because there are no reasons provided for why the retrieval context is irrelevant to the input, indicating a complete lack of relevance.",
Contextual Relevancy,0.3333333333333333,Llama-3 70B,The score is 0.33 because the context does not mention ,
Contextual Relevancy,1.0,Llama-3 70B,"The score is 1.00 because the retrieval context is perfectly relevant to the input, providing a direct answer to the question.",
Contextual Relevancy,1.0,Llama-3 70B,"The score is 1.00 because the input is directly related to the context, making it perfectly relevant.",
Contextual Relevancy,1.0,Llama-3 70B,"The score is 1.00 because the input is directly answered by the retrieval context, making it perfectly relevant.",
Contextual Relevancy,1.0,Llama-3 70B,"The score is 1.00 because the retrieval context perfectly aligns with the input, providing a precise answer to the question about the Government Property.",
Contextual Relevancy,1.0,Llama-3 70B,"The score is 1.00 because the retrieval context is perfectly aligned with the input, providing a precise answer to the question asked.",
Contextual Relevancy,0.0,Llama-3 70B,The score is 0.00 because there is no relevant information provided in the context to relate to the input.,
Contextual Relevancy,1.0,Llama-3 70B,"The score is 1.00 because the input and retrieval context are perfectly aligned, and the context provides a direct answer to the input question.",
Contextual Relevancy,1.0,Llama-3 70B,,
Contextual Relevancy,1.0,Llama-3 70B,"The score is 1.00 because the input is directly answered by the retrieval context, making it perfectly relevant.",
Contextual Relevancy,1.0,Llama-3 70B,"The score is 1.00 because the retrieval context is perfectly aligned with the input, providing a direct answer to the question asked.",
Contextual Relevancy,1.0,Llama-3 70B,,
Contextual Relevancy,1.0,Llama-3 70B,"The score is 1.00 because the input directly asks about editing the context of an entry in ios, and there are no reasons provided for why the retrieval context is irrelevant to the input, indicating a perfect match.",
Contextual Relevancy,1.0,Llama-3 70B,"The score is 1.00 because the input and retrieval context are perfectly aligned, making the context extremely relevant!",
Contextual Relevancy,0.5,Llama-3 70B,"The score is 0.50 because the context does not mention torques or orthogonal axes, and instead talks about unrelated topics, making it only partially relevant to the input question.",
Contextual Relevancy,1.0,Llama-3 70B,"The score is 1.00 because the input is a simple question and the context is empty, making it perfectly relevant.",
Contextual Relevancy,1.0,Llama-3 70B,"The score is 1.00 because there are no reasons for irrelevancy provided, indicating a perfect match between the input and retrieval context.",
Contextual Relevancy,1.0,Llama-3 70B,"The score is 1.00 because the input is perfectly matched with the context, providing an exact answer to the question.",
Contextual Relevancy,1.0,Llama-3 70B,"The score is 1.00 because the retrieval context is perfectly aligned with the input, providing a precise answer to the question about spacecraft design during an interplanetary mission.",
Contextual Relevancy,1.0,Llama-3 70B,,
Contextual Relevancy,1.0,Llama-3 70B,"The score is 1.00 because this question is perfectly matched to the retrieval context, with no reasons found for irrelevancy.",
Contextual Relevancy,1.0,Llama-3 70B,"The score is 1.00 because the input is perfectly aligned with the context, providing a direct answer to the question asked.",
Contextual Relevancy,1.0,Llama-3 70B,"The score is 1.00 because the input is directly addressed by the context, providing a perfect match with no irrelevant information present.",
Contextual Relevancy,1.0,Llama-3 70B,,
Contextual Relevancy,0.5,Llama-3 70B,"The score is 0.50 because the context does not mention the backshield of a silicon solar cell, instead discussing unrelated topics like maximum power point tracker and battery discharge regulators, making it only partially relevant to the input question.",
